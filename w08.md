---
layout: "layout"
permalink: /W08/
---

# Top 10 List of Week 08

1. [What is the difference between adduser and useradd? (thread)](https://askubuntu.com/questions/345974/what-is-the-difference-between-adduser-and-useradd)<br>
This surprisingly halt my Linux From Scratch experience, as they are both similar commands, yet one is much better than other. From this article, I learned that useradd is native binary compiled with the system while adduser is a perl script which uses useradd binary in back-end. TLDR, use adduser, as it will handle things such as password creation and home directory creation and assignment. (I've tried with useradd, even when you've created the home directory, you won't be assigned with it).

2. [Difference between Asymmetric and Symmetric Multiprocessing (article)
](https://www.geeksforgeeks.org/difference-between-asymmetric-and-symmetric-multiprocessing/)<br>
Inspired by the question asked in class, I wanted to learn more about the difference between asymmetric and symmetric multiprocessing. Articles made by G4G are always great, this one's no different. This article has diagrams as well as tables to help understand the difference between these two multiprocessing types. In asymmetric multiprocessing, only the master processor runs the tasks of an OS. Symmteric multiprocessing however, every processors are treated equally.

3. [CPU Scheduling : Finding burst time (thread)](https://stackoverflow.com/questions/24848864/cpu-scheduling-finding-burst-time)<br>
What is CPU burst? CPU burst: the amount of time the process uses the processor before it is no longer ready, and then what?. This stackoverflow article is a great repository for even more links, as shared by the top voted answer. This article discusses the topic of finding burst time. From my understanding, schedulers estimate the lenght of the next burst based on recent cpu bursts. It is basically algorithmic guessing by assuming every event is related (non-independent). Second top answers also gives the formula
- Estimated CPU Burst time for (n+1)th CPU burst=(alpha)(Actual CPU Burst time for nth CPU Burst)+(1-alpha)(Estimated CPU Burst time for nth CPU Burst).

4. [Round Robin scheduling (thread)](https://stackoverflow.com/questions/4081999/round-robin-scheduling)<br>
This article is interesting, as it takes on a more university-test-problem question. Thread starter asked the following question, given the recent CPU and I/O bursts and their respective length in time units, describe it's state transition. Round robin is very popular in learning about scheduling and load balancing, but it has it's flaws such as uneven work deistribution. I learned a bit more about the theoretical side of this load balancer.

5. [Introduction to CPU Scheduling (video)](https://www.youtube.com/watch?v=EWkQl0n0w5M)<br>
Neso Academy is my go-to youtuber for digital systems, computer organization, and now operating systems. Their videos are complete and detailed, this one's no different. I now understand that CPU scheduling is the time we are assigning to different processes for their utilization of the CPU. We don't want the CPU to remain idle, I.E when the CPU is waiting for something to complete. By assigning the CPU to another process during that waiting time, we reduce idle time and maximize the utilization of CPU. 

6. [CPU and I/O Burst Cycles (video)](https://www.youtube.com/watch?v=pVzb3TUcDLo)<br>
Now that I've understand CPU scheduling, the next step is to learn about CPU and I/O burst cycles. Video is also from NesoAcademy, very detailed, but I'll try to summarize what I've just learned. A process can be in either two states: `CPU execution state` or `I/O wait`. They also alternate between each other. CPU bursts is the `time` when the process is in CPU execution state. The same goes for I/O bursts.

7. [Load Balancer â€“ System Design Interview Question (article)](https://www.geeksforgeeks.org/load-balancer-system-design-interview-question/)<br>
For the rest of this week's links, I'll be trying to learn this week's topics in a software engineering approach, as I feel like it would benefit me the most and help me get ready in interviews. First up is Load Balancers, I've heard about L4 and L7 load balancers, but what are they exactly? G4G was the right choice to learn this, as this article is very complete. L4 lies in OSI model layer 4, which is the transport layer (TCP/SSL) therefore L4 is also referred to as `Network Load Balancing`. L7 on the other hand lies in OSI model layer (HTTP/HTTPS) and therefore is referred to as `Application Load Balancer` or `HTTP(s) load balancer`. Finally learned the difference!

8. [Load balancing: system design interview concepts (4 of 9) (article)](https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview)<br>
This website is actually gold, and I'm glad I've stumbled upon it. Previously we've only discussed the difference of L4 and L7 load balancer, now we will take a deeper dive on this topic. There are two types of Load Balancing:
- Static: simpler and easier to implement, but causes uneven workload distribution. `Round robin` is a great example. `Weighted Round Robin` utilizes the server with better resources to handle the bigger request (ex: a server dedicated for video processing), yet it still has the same flaws with the original round robin, uneven work distribution. Other examples are `Random algorithm` and `User IP Hashing`.
- Dynamic: `Least Load Algorithm` addresses the problems that static load balancing has, that is to allocate each request to the server that has the least load at the current time. This is obviously more complex to implement. Another great one to learn is `Power-of-d-choices`, which is efficient when `d` is small, but only works when `d` is approximately the number of servers.

9. [CFS: Completely fair process scheduling in Linux (article)](https://opensource.com/article/19/2/fair-scheduling-linux)<br>
This article is very interesting, as it offers more than it's title suggests. I first heard about preemptive scheduling in class, which works by having a fixed amount of time that a task is allowed to hold a processor until preempted in favor of some other task. A typical model also has explicit prorities, meaning higher-priority processes gets scheduled first. CFS on the other hand, computes the amount of time of a given task dynamically. CFS scheduler as a target latency, and each runnable task would get 1/N slice of the target latency. Here, N is the number of tasks.  The fair in CFS comes from the even distribution of the slice (1/N) for each N tasks.

10. [Cracking system design interviews guide (article)](https://igotanoffer.com/blogs/tech/system-design-interviews)<br>
This is a complete and very lengthy list, so sit back and take your time reading it. This super article is interesting, because of the amount of information it offers. Here are the topics touched up upon this article:
- Network protocols and proxies
- Databases
- Latency, throughput, and availability
- Load balancing
- Leader election
- Caching
- Sharding
- Polling, SSE, and WebSockets
- Queues and pub-sub
Pretty insane! The amount of OS topics to learn in a software engineering standpoint is amazing, and it definitely helped me be more confident in my next interview. I've also learned alot about leader election, which is when you want to have a more fine tuned scaling, rather than horizontally scaling.
